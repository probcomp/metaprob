{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "; If using Jupyter Lab, run\n",
    ";   jupyter labextension install @jupyterlab/javascript-extension\n",
    "; before using this notebook.\n",
    "(ns tutorial\n",
    "    (:refer-clojure :only [])\n",
    "    (:require\n",
    "        [clojure.repl :refer :all]\n",
    "        [metaprob.state :as state]\n",
    "        [metaprob.trace :as trace]\n",
    "        [metaprob.sequence :as sequence]\n",
    "        [metaprob.builtin-impl :as impl]\n",
    "        [metaprob.syntax :refer :all]\n",
    "        [metaprob.builtin :refer :all]\n",
    "        [metaprob.prelude :refer :all]\n",
    "        [metaprob.distributions :refer :all]\n",
    "        [metaprob.interpreters :refer :all]\n",
    "        [metaprob.inference :refer :all]\n",
    "        [metaprob.compositional :as comp]\n",
    "        [metaprob.examples.gaussian :refer :all]\n",
    "        [metaprob.tutorial.jupyter :refer :all]))\n",
    "\n",
    "(enable-inline-viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Metaprob!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metaprob is a new, experimental language for probabilistic programming, embedded in Clojure. In this tutorial, we'll take a bottom-up approach to introducing the language. We'll begin by exploring Metaprob's core data types, including the flexible \"trace\" data structure at the heart of the language's design. From there, we'll look at the probabilistic semantics of Metaprob programs, and tackle some small statistical inference problems. By the end of this tutorial, we'll be querying a more sophisticated model of real US census data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Metaprob values and the trace data structure\n",
    "The Metaprob language is embedded in Clojure, and inherits several of Clojure's atomic datatypes: Clojure's numbers, keywords, booleans, and strings are all Metaprob values, too. Many of Clojure's functions for manipulating these values have Metaprob versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(+ 5 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use a Clojure function that is not available in Metaprob, you can fully qualify the procedure's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(clojure.core/str \"Hello\" \" \" \"world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Any Clojure function can be used without modification in Metaprob. For reasons we'll cover a bit later, however, calling impure Clojure functions that have nondeterministic behavior (e.g., because they generate random values during their execution) must be done with care, in order not to break Metaprob's probabilistic semantics. Note that this restriction applies also to higher-order Clojure functions supplied with stochastic procedures as arguments.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, it appears that Metaprob supports Clojure's vector and list datatypes, too. Here, we apply Metaprob's `length` function to a vector, then to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(length [1 5 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(length (list 1 2 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But really, lists and vectors are just special cases of Metaprob's \"trace\" data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(and (trace? [1 2 3]) (trace? '(1 2 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(plot-trace [5 2 9] 200 200) ; produce a 200x200 diagram of the given trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(plot-trace '(5 2 9) 500 100) ; produce a 500x100 diagram of the given trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trace is a tree-like data structure. Every trace:\n",
    "* has a root node (the leftmost gray box in the diagrams above);\n",
    "* optionally stores a value at the root node (displayed inside that gray box); and\n",
    "* has zero or more named subtraces (connected to the root by thin edges in the diagrams above). Each subtrace has its own root node and, potentially, its own named subtraces.\n",
    "\n",
    "Subtraces may be named with strings, numbers, or booleans. The name of each subtrace appears above its root node in the diagrams above.\n",
    "\n",
    "So far, you have seen two special kinds of trace:\n",
    "\n",
    "1. A _vector_ is a trace with no value at its root, and `n` subtraces, labeled by the integers `0` through `n-1`. In a vector, each subtrace stores a value at its root, and has no children.\n",
    "\n",
    "2. A _list_ is either:\n",
    "    - The empty trace (a root node with no value or children), or\n",
    "    - A trace with a value at its root, and a single subtrace, called \"rest\", which itself is a list.\n",
    "\n",
    "But most traces are not vectors or lists. Consider the following trace, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(plot-trace (gen [x y] (+ x (* 3 y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have guessed, the above value is a Metaprob procedure. (The `gen` macro, which we'll cover in more detail soon, is Metaprob's version of Clojure's `fn`, Scheme's `lambda`, or JavaScript's anonymous `function`.) In Metaprob, all compound data, including procedures, are stored as traces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Constructing and manipulating traces\n",
    "\n",
    "Let's look at some of the tools Metaprob provides for manipulating and creating traces.\n",
    "\n",
    "We'll start with trace _manipulation_. We'll use the trace depicted above as a running example. Let's give it a name using the Metaprob keyword `define`. Below, we also demonstrate the `pprint` function, which is another way (apart from `plot-trace`) to get a human-readable representation of a trace. You can ignore the word `COMPILED` for now: it just indicates that the Metaprob has produced an efficient Clojure-compiled version of the procedure that this trace represents (something the `gen` macro is also responsible for)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define example-trace (gen [x y] (+ x (* 3 y))))\n",
    "(pprint example-trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Reading values in traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most basic things we can do with a trace is get its root value, using `trace-get`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trace-get example-trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all traces have values at their roots, and `trace-get` will throw an error if there is no value for it to return. So before you call `trace-get`, it is sometimes useful to call `trace-has?`, which checks if a trace has a root value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Recall that a vector is represented\n",
    "; as a trace with no value at its root,\n",
    "; and subtraces for each of its elements.\n",
    "(trace-has? [1 2 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `trace-has?` and `trace-get` accept an optional second argument: an _address_. An address is either:\n",
    "- the name of a subtrace, or\n",
    "- a list of names, representing a \"path\" to the desired subtrace.\n",
    "\n",
    "This version of `trace-has?` returns false if the given address is invalid for the trace, or if it is valid but the specified subtrace has no value at its root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(plot-trace [5 7 9] 200 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Get the value at the subtrace named 2\n",
    "(trace-get [5 7 9] 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; There is no subtrace 4\n",
    "(trace-has? [5 7 9] 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; There is no subtrace \"rest\"\n",
    "(trace-has? '() \"rest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; The subtrace \"rest\" exists, but has no value at its root\n",
    "(trace-has? '(1) \"rest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; The subtrace exists and has a value (2)\n",
    "(trace-has? '(1 2) \"rest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Returns the _value_ at the subtrace named \"rest\" -- 2 -- and not \n",
    "; the subtrace itself -- which would be '(2).\n",
    "(trace-get '(1 2) \"rest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Gets the value in the \"environment\" subtrace.\n",
    "(trace-get example-trace \"environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Gets the _value_ in the 0 subtrace of the \"body\" subtrace of the \"generative-source\" subtrace\n",
    "(trace-get example-trace '(\"generative-source\" \"body\" 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; There is no child subtrace called 4 of the '(\"generative-source\" \"body\") subtrace.\n",
    "(trace-has? example-trace '(\"generative-source\" \"body\" 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: `trace-get-maybe` for safe access\n",
    "In this exercise, you will write your first Metaprob function! \n",
    "\n",
    "To define a function, use the form\n",
    "```\n",
    "(define func-name\n",
    "  (gen [arg1 arg2 ...]\n",
    "    body))\n",
    "```\n",
    "\n",
    "In the box below, define `(trace-get-maybe tr adr default)`, which should return the value of `tr` at `adr`, or `default` if no value exists at that address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Solution\n",
    "(define trace-get-maybe\n",
    "  (gen [tr adr default]\n",
    "    (if (trace-has? tr adr) (trace-get tr adr) default)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: `vec-ref`\n",
    "Write a function `(vec-ref v n)` that takes in a vector (a trace with subtraces labeled 0, 1, ...), and an element number `n`, and returns the `n`th element of the vector. If `n` is out-of-bounds for the vector, return the Clojure keyword `:out-of-bounds`.\n",
    "\n",
    "As in Clojure, the body can contain multiple expressions, in which case they will be evaluated in turn and the last result will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Solution\n",
    "(define vec-ref\n",
    "  (gen [v n]\n",
    "    (trace-get-maybe v n :out-of-bounds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB**: `trace-has?` does _not_ check whether a particular address _exists_ in the trace, just whether there is a _value_ at that trace. It could return false for the address `'(\"a\" \"b\")` but not for `'(\"a\" \"b\" \"c\")`. Similarly, `trace-get` does not return an entire subtrace at an address, but just the value at that address. `(trace-get example-trace \"generative-source\")` returns the string `\"gen\"`, not the entire subtrace representing the `example-trace` source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Reading subtraces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check for or retrieve a _subtrace_ at an address, use `trace-has-subtrace?` and `trace-subtrace`. Unlike `trace-has?` and `trace-get`, these two functions _require_ a second argument, which specifies the address of the subtrace in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(plot-trace example-trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Extract the generative-source subtrace of our example.\n",
    "(plot-trace (trace-subtrace example-trace \"generative-source\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Extraction at a longer address.\n",
    "(plot-trace (trace-subtrace example-trace '(\"generative-source\" \"body\" 2)) 300 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: N-D arrays\n",
    "We can represent an $n$-dimensional array in Metaprob using vectors of vectors. For example, a matrix (2D array) would be a vector of _matrix rows_, each of which is itself a vector of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define example-2d-matrix [[1 2 3] \n",
    "                           [4 5 6] \n",
    "                           [7 8 9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `(n-d-vec-ref mat indices)` which takes in\n",
    "- an $n$-dimensional matrix `mat`, represented as a vector of vectors (of vectors, of vectors, ...), and\n",
    "- a Metaprob list of indices\n",
    "and returns either `:out-of-bounds` (if the indices push beyond the array's bounds) or the value at the given address.\n",
    "\n",
    "(Food for thought: why doesn't `(trace-get v indices)` suffice to implement this operation? If confused, try using `plot-trace` on an N-D vector.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Solution\n",
    "(define n-d-vec-ref\n",
    "  (gen [v indices]\n",
    "    ; If `indices` is non-empty:\n",
    "    (if (trace-has? indices)\n",
    "        ; Then check whether the first index exists as a subtrace of v\n",
    "        (if (and (trace? v) (trace-has? v (trace-get indices)))\n",
    "            ; If it does, pull it out and make a recursive call\n",
    "            (n-d-vec-ref (trace-get v (trace-get indices)) (trace-subtrace indices \"rest\"))\n",
    "            ; If it doesn't, we're out of bounds\n",
    "            :out-of-bounds)\n",
    "        ; If `indices` is empty, we're done: return v\n",
    "        v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Testing: should output true.\n",
    "(and\n",
    "    (= 3 (n-d-vec-ref example-2d-matrix '(0 2)))\n",
    "    (= 7 (n-d-vec-ref example-2d-matrix '(2 0)))\n",
    "    (= :out-of-bounds (n-d-vec-ref example-2d-matrix '(1 1 2)))\n",
    "    (= :out-of-bounds (n-d-vec-ref example-2d-matrix '(1 3)))\n",
    "    (= :out-of-bounds (n-d-vec-ref example-2d-matrix '(3 1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Refactoring trace access code\n",
    "\n",
    "In a Metaprob program she is working on, Alice has written the following expression:\n",
    "\n",
    "```\n",
    "(trace-subtrace (trace-subtrace (trace-get flip \"implementation\") \"generative-source\") \"body\")\n",
    "```\n",
    "\n",
    "This code is correct, but during code review, it strikes Bob as inelegant. He rewrites it as follows, taking advantage of the fact that functions like `trace-get` accept arbitrarily long \"paths\" of subtrace names as _addresses_:\n",
    "\n",
    "```\n",
    "(trace-subtrace flip '(\"implementation\" \"generative-source\" \"body\"))\n",
    "```\n",
    "\n",
    "He is so pleased with the elegance of his code that he commits without testing. Oops!\n",
    "\n",
    "Why doesn't Bob's rewrite work, when Alice's original code did? Can Alice's code still be made more concise in another way?\n",
    "\n",
    "(Note: `flip` is a real Metaprob value, so you can run both snippets -- and your own proposed rewrite -- to test them out.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Solution\n",
    "; `flip` has a subtrace called \"implementation\", but that subtrace has no children. In particular, \"generative-source\" is not a child of the \"implementation\" subtrace. \n",
    "; So the address '(\"implementation\" \"generative-source\") is meaningless for `flip`.\n",
    "; Instead, the value stored at the root of `flip`'s \"implementation\" subtrace is itself a trace.\n",
    "; Alice's code can be simplified somewhat, to:\n",
    "(trace-subtrace (trace-get flip \"implementation\") '(\"generative-source\" \"body\"))\n",
    "; because '(\"generative-source\" \"body\") is exactly the address that calling `(trace-subtrace (trace-subtrace ... \"generative-source\") \"body\")` (as Alice did) probes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Run this cell to visualize\n",
    "(plot-trace (trace-get flip \"implementation\") 2000 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Traversing nested traces\n",
    "\n",
    "In the exercise above, you saw a situation in which a trace's root value was itself a trace. Write `flattened-trace-has-subtrace?` and `flattened-trace-subtrace`, which work just like `trace-has-subtrace?` and `trace-subtrace`, with the following exception: if you reach a \"dead end\" while traversing an address, check to see if the value stored at that dead end is itself a trace (using `trace?`), and if so, continue traversing as if it were a subtrace.\n",
    "\n",
    "This will enable you to write, e.g., `(flattened-trace-subtrace flip '(\"implementation\" \"generative-source\" \"body\"))`, or `(flattened-trace-subtrace example-2d-matrix '(1 1))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Solution\n",
    "\n",
    "(define flattened-trace-has-subtrace?\n",
    "  (gen [tr adr]\n",
    "    (or\n",
    "        ; address is empty: we are already at the subtrace\n",
    "        (not (trace-has? adr))\n",
    "        ; the first piece of the address exists as a direct subtrace,\n",
    "        ; and the recursive call succeeds\n",
    "        (and\n",
    "            (trace-has-subtrace? tr (trace-get adr))\n",
    "            (flattened-trace-has-subtrace? (trace-subtrace tr (trace-get adr)) (trace-subtrace adr \"rest\")))\n",
    "        ; our current node stores a trace, for which (flattened-trace-has-subtrace? . adr) succeeds.\n",
    "        (and\n",
    "            (trace-has? tr)\n",
    "            (trace? (trace-get tr))\n",
    "            (flattened-trace-has-subtrace? (trace-get tr) adr)))))\n",
    "\n",
    "(define flattened-trace-subtrace\n",
    "  (gen [tr adr]\n",
    "    (cond \n",
    "        (not (trace-has? adr))\n",
    "        tr\n",
    "        \n",
    "        (trace-has-subtrace? tr (trace-get adr))\n",
    "        (flattened-trace-subtrace (trace-subtrace tr (trace-get adr)) (trace-subtrace adr \"rest\"))\n",
    "         \n",
    "        (and (trace-has? tr) (trace? (trace-get tr)))\n",
    "        (flattened-trace-subtrace (trace-get tr) adr))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Setting (and clearing) values and subtraces\n",
    "\n",
    "Metaprob provides two functions, `(trace-set tr [adr] val)` and `(trace-set-subtrace tr adr sub)`, which return modified versions of traces:\n",
    "- `(trace-set tr val)` changes the value of the trace `tr`'s root node to `val`.\n",
    "- `(trace-set tr adr val)` changes the _root value_ of the subtrace at the address `adr` to `val`.\n",
    "- `(trace-set-subtrace tr adr sub)` changes the entire subtrace of `tr` at `adr` to `sub`.\n",
    "\n",
    "These functions do not change the original trace, but rather create a _copy_ of the trace with some value or subtrace changed. Metaprob does have mutable traces, and operators like `trace-set!` and `trace-set-subtrace!` that operate on them, but we will not have a need for them in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Predict the results.\n",
    "\n",
    "Predict the results of each of the following expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(clojure.core/refer-clojure :only '[println])\n",
    "\n",
    "; 1.\n",
    "(println \"Problem 1\")\n",
    "(pprint (trace-set [5 7 9] 0))\n",
    "\n",
    "; 2.\n",
    "(println \"Problem 2\")\n",
    "(pprint (trace-set [5 7 9] 0 0))\n",
    "\n",
    "; 3.\n",
    "(println \"Problem 3\")\n",
    "(pprint (trace-set [5 7 9] 5 2))\n",
    "\n",
    "; 4.\n",
    "(println \"Problem 4\")\n",
    "(pprint (trace-set-subtrace '(1 2 3 4) '(\"rest\" \"rest\") '(5 6 7)))\n",
    "\n",
    "; 5.\n",
    "(println \"Problem 5\")\n",
    "(pprint (trace-set-subtrace (gen [] 5) '(\"generative-source\" \"pattern\")\n",
    "                            (trace-subtrace (gen [x y z] (+ x y z))\n",
    "                                            '(\"generative-source\" \"pattern\"))))\n",
    "\n",
    "; 6.\n",
    "(println \"Problem 6\")\n",
    "(pprint (trace-set-subtrace [5 7 9] 2 '(5 7 9)))\n",
    "\n",
    "; 7.\n",
    "(println \"Problem 7\")\n",
    "(pprint (trace-set [5 7 9] 2 '(5 7 9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a `trace-delete` function, which clears the value in a trace's root node. Optionally, it can take an address; in that case, it clears the _root value_ of the subtrace at that address (but does _not_ delete the subtrace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(plot-trace (trace-delete [5 7 9] 2) 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, notice that although the `2` subtrace has no value, it still exists in the trace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Constructing traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(trace ...)` is Metaprob's all-purpose constructor for traces. If you pass it no arguments, you get the empty trace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(plot-trace (trace) 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to construct a bigger trace is to start out with an empty one and use `trace-set` and `trace-set-subtrace` to add values and sub-traces. This can be quite tedious, so `(trace ...)` also supports an argument syntax that is demonstrated in the examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Trace with a root value\n",
    "(plot-trace (trace :value 10) 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Trace with subtraces with root values\n",
    "(plot-trace (trace \"Massachusetts\" \"Boston\", \"California\" \"Sacramento\", \"New York\" \"New York\") 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Trace with root value and subtraces with root values\n",
    "(plot-trace\n",
    "    (trace :value \"John Doe\", \"first name\" \"John\", \"last name\" \"Doe\", \"address\" \"500 Penn Ave.\", \"age\" 94)\n",
    "    400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Trace with arbitrary named subtrace, using (** ...)\n",
    "(plot-trace\n",
    "    (trace \"a\" (** (trace \"b\" \"c\")) \"b\" 5) \n",
    "  300 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Trace with multiple named subtraces and a value\n",
    "(plot-trace\n",
    "    (trace :value \"phrases\", \n",
    "           \"green\" (** (trace :value \"first word\",\n",
    "                              \"eggs\" (** (trace \"and\" (** (trace \"ham\" \"!\")))),\n",
    "                              \"peace\" \".\",\n",
    "                              \"ideas\" (** (trace \"sleep\" (** (trace \"furiously\" \"?\"))))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Binary search tree\n",
    "\n",
    "As a way to get used to these trace operations, in this exercise, we'll implement binary search trees in Metaprob using traces. For our purposes, a binary search tree is either:\n",
    "- The empty trace (representing an empty tree)\n",
    "- A trace with a number at its root, and (optionally) \"left\" and \"right\" subtraces which are themselves binary trees. All values in the left subtrace should be less than or equal to the root's value, and all values in the right subtrace should be greater than or equal to the root's value.\n",
    "\n",
    "For this exercise, write:\n",
    "1. A manually constructed binary search tree, using the `(trace ...)` function, with the numbers 1 through 10. (There are multiple valid binary search trees containing these numbers; any organization that is consistent with the above rules is fine.)\n",
    "2. A function `(insert tree i)`, which inserts a number `i` into `tree`.\n",
    "3. A function `(contains? tree i)` which checks whether a tree contains a given node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Solution\n",
    "\n",
    "(define example-tree\n",
    "  (trace :value 5\n",
    "         \"left\" (** (trace :value  3\n",
    "                           \"left\"  (** (trace :value 1 \"right\" 2))\n",
    "                           \"right\" 4))\n",
    "         \"right\" (** (trace :value  7\n",
    "                            \"left\" 6\n",
    "                            \"right\" (** (trace :value 9 \"left\" 8 \"right\" 10))))))\n",
    "\n",
    "(define insert \n",
    "  (gen [tree i]\n",
    "  (if (empty-trace? tree)\n",
    "      (trace-set tree i)\n",
    "      (block\n",
    "        (define branch (if (<= i (trace-get tree)) \"left\" \"right\"))\n",
    "        (trace-set-subtrace\n",
    "          tree\n",
    "          branch\n",
    "          (insert (if (trace-has-subtrace? tree branch)\n",
    "                      (trace-subtrace tree branch)\n",
    "                      (trace)) i))))))\n",
    "\n",
    "(define contains?\n",
    "    (gen [tree i]\n",
    "         (and (not (empty-trace? tree))\n",
    "              (or\n",
    "                  (= (trace-get tree) i)\n",
    "                  (block\n",
    "                      (define branch (if (<= i (trace-get tree)) \"left\" \"right\"))\n",
    "                      (and (trace-has-subtrace? tree branch)\n",
    "                           (contains? (trace-subtrace tree branch) i)))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(clojure.core/every? (gen [n] (contains? example-tree (+ n 1))) (range 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Traversing traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've now written several functions that traverse traces recursively, but you always knew the general structure of the trace you were traversing. For cases when you don't know ahead-of-time how many children a trace will have -- or what its subtraces will be called -- the `(trace-keys tr)` function comes in handy. It lists the names of all subtraces of a given trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trace-keys example-trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Metaprob has a version of Clojure's `map` function, which often comes in handy when dealing with lists of subtrace names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Get values at each subtrace\n",
    "(map (gen [name] (trace-get example-trace name)) (trace-keys example-trace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metaprob also has `apply`, which applies a procedure to a list of arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Concatenate the names of a trace's children\n",
    "(apply clojure.core/str (trace-keys example-trace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Automatic sizing for trace visualizations\n",
    "Currently, the `plot-trace` function draws trace diagrams of whatever size the user specifies. In this exercise, you'll write a function `smart-plot-trace` that automatically decides on a size for the trace, based on its breadth and depth:\n",
    "\n",
    "1. Begin by defining `(trace-depth tr)`, which recursively computes the maximum depth of a trace.\n",
    "2. Next, define `(trace-breadth tr i)`, which gives the trace's breadth at level `i`. (Hint: sum the widths of the sub-traces at level `i-1`.)\n",
    "3. Define `(max-trace-breadth tr)` which gives the maximum breadth of a trace at any of its levels. (Note: the implementation we are suggesting in this exercise, which simply calls `trace-breadth` once for each level of the trace, is $O(n^2)$; there are certainly more efficient algorithms!)\n",
    "4. Write a function `smart-plot-trace` that calls `plot-trace`, passing in reasonable values for diagram width and height (based on the depth and breadth of the trace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Bring in some useful Clojure functions/macros\n",
    "(clojure.core/refer-clojure :only '[empty? max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Solution\n",
    "(define trace-depth\n",
    "  (gen [tr]\n",
    "    (if (empty? (trace-keys tr))\n",
    "        1\n",
    "        (apply max \n",
    "            (map (gen [k]\n",
    "                     (+ 1 (trace-depth (trace-subtrace tr k)))) (trace-keys tr))))))\n",
    "\n",
    "(define trace-breadth\n",
    "   (gen [tr i]\n",
    "     (if (= i 0)\n",
    "         1\n",
    "         (apply \n",
    "             + \n",
    "             (map (gen [k]\n",
    "                     (trace-breadth (trace-subtrace tr k) (- i 1))) (trace-keys tr))))))\n",
    "\n",
    "(define max-trace-breadth\n",
    "   (gen [tr]\n",
    "     (apply max (map (gen [i] (trace-breadth tr i)) (range (trace-depth tr))))))\n",
    "\n",
    "(define smart-plot-trace\n",
    "   (gen [tr]\n",
    "     (plot-trace tr (* (trace-depth tr) 100)\n",
    "                    (* (max-trace-breadth tr) 80))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(smart-plot-trace (gen [x y] (+ x (* 3 y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(smart-plot-trace (trace-get flip '(\"implementation\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(smart-plot-trace\n",
    "  (nth (infer\n",
    "        :procedure comp/infer-apply\n",
    "        :inputs [(gen [] (flip 0.5)) [] (trace) (trace) true]) 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge Exercise: Delete subtrace\n",
    "\n",
    "Write a function `(delete-subtrace tr adr)` that completely deletes the addressed subtrace from a trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge Exercise: All addresses with values\n",
    "Write a function `(all-addresses-with-values tr)` which returns a list of addresses at which `tr` has a value. (The built-in function `addresses-of` can already do this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define trace-set-values-at\n",
    "  (gen [t & pairs]\n",
    "    (if (empty? pairs)\n",
    "        t\n",
    "        (block\n",
    "          (define addr (first pairs))\n",
    "          (define value (first (rest pairs)))\n",
    "          (define others (rest (rest pairs)))\n",
    "          (apply trace-set-values-at (pair (trace-set t addr value) others))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Probabilistic modeling, random choices, and trace-based inference\n",
    "\n",
    "So far, every procedure we've written has been _deterministic_: the outputs are completely determined by the inputs, and are the same every time we run the code. Now, we turn our attention to _non-deterministic_ procedures, with which we can do the sort of probabilistic modeling that Metaprob was made for.\n",
    "\n",
    "## 3.1 A few simple models\n",
    "\n",
    "There are many interesting questions that can be phrased using the language of probability:\n",
    "- How much can I expect my lifetime earnings to improve if I get a college degree?\n",
    "- What is the probability that this treatment will work for this patient?\n",
    "- With what confidence can we say that these two documents were produced by the same author?\n",
    "\n",
    "Before we can even begin to answer these questions, though, we need a _model_: a (sometimes simplified) mathematical description of how we imagine the real-world processes behind these quesitons (the job market, human biology, authorship) actually work. \n",
    "\n",
    "One of the key insights in probabilistic programming is that writing a program to simulate a process is a good way to specify our probabilistic models.\n",
    "\n",
    "A few examples will make this clearer.\n",
    "\n",
    "### Scenario 1: A tricky coin\n",
    "Suppose you are interviewing for a position at a financial firm, and your interviewer produces a coin. He flips the coin 100 times, and you observe 100 heads. What is the probability, he asks you, that the next flip, too, will come up heads?\n",
    "\n",
    "Your answer will depend on your _model_ of the random process you're observing.\n",
    "\n",
    "Here is one possible model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define coin-model-1\n",
    "  (gen []\n",
    "    (replicate 100 (gen [] (flip 0.5)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have used `replicate`, which takes a number (100, in this case) and a 0-argument procedure, and runs the procedure that many times, returning a list of the results from each run. In this model, we hypothesize that the underlying process at work is as follows: a fair coin is flipped 100 times. We are 100% sure that the coin is fair: hence the constant `0.5`.\n",
    "\n",
    "Another model might not be so trusting of the interviewer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define coin-model-2\n",
    "  (gen []\n",
    "    (define p (uniform 0 1))\n",
    "    (replicate 100 (gen [] (flip p)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we imagine that the interviewer randomly decided on the coin's weight (a number between 0 and 1) before we entered the room, and procured a biased coin to flip for us.\n",
    "\n",
    "Perhaps we don't really believe that's possible, though: how does one get a \"weighted coin\" anyway? Maybe we really believe only three possibilities going in: the coin is fair, a \"heads\" is painted on both sides, or a \"tails\" is painted on both sides. Let's try to model that scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define coin-model-3\n",
    "  (gen []\n",
    "    (define p (uniform-sample [0 0.5 1]))\n",
    "    (replicate 100 (gen [] (flip p)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Gambler's fallacy\n",
    "\n",
    "The \"gambler's fallacy\" is (according to Wikipedia) \"the mistaken belief that, if something happens more frequently than normal during a given period, it will happen less frequently in the future.\" Create a model `coin-model-4` that posits the existence of a \"God-like\" figure who adjusts the weight of the coin after each flip to help ensure, at every step, that the whole _sequence_ of flips will be as evenly split between Heads and Tails as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Solution\n",
    "\n",
    "(define reduce\n",
    "  (gen [f start l]\n",
    "    (if (empty-trace? l)\n",
    "        start\n",
    "        (reduce f (f start (first l)) (rest l)))))\n",
    "\n",
    "(define coin-model-4\n",
    "  (gen []\n",
    "    (reduce \n",
    "      (gen [l n]\n",
    "        (define current-ratio (/ (length (filter (gen [x] x) l)) n))\n",
    "        (pair (flip (- 1 current-ratio)) l))\n",
    "        (list (flip 0.5))\n",
    "        (map (gen [x] (+ x 1)) (range 99)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2: The Wage Gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research shows that men are consistently paid more than women for the same work ([OECD](https://data.oecd.org/earnwage/gender-wage-gap.htm)), and also that no matter their gender, taller people are paid more ([Case and Paxson, 2006](http://www.nber.org/papers/w12466.pdf)). Of course, gender and height are correlated: men tend to be taller than women. That research in hand, we may want to answer questions like:\n",
    "- If all we know about someone is that they are 6' tall, what might we expect their salary to be? Their sex?\n",
    "- What is the probability that a woman's salary will be within some range?\n",
    "- and many more...\n",
    "\n",
    "We can encode the research on the subject into a model. Below, the numbers for average height and for the relationship beteween height and pay is from [Case and Paxson](http://www.nber.org/papers/w12466.pdf). The median salary data for men and women is from [the Bureau of Labor Statistics](https://www.bls.gov/opub/ted/2014/ted_20140423.htm). That said, this is still a highly simplified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define wage-gap-model\n",
    "  (gen []\n",
    "    (define sex (uniform-sample [:m :f]))\n",
    "    (define height-mean (if (= sex :m) 70 64))\n",
    "    (define height (gaussian height-mean 4))\n",
    "    (define base-salary-mean (if (= sex :m) 44600 36920))\n",
    "    (define height-adjusted-salary-mean (* base-salary-mean (exp (* (- height height-mean) (if (= sex :m) 0.024 0.019)))))\n",
    "    (define salary (gaussian height-adjusted-salary-mean 2000))\n",
    "    salary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(wage-gap-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Monte Carlo estimation for expectations\n",
    "\n",
    "Many queries can be formed as questions about averages. For example, we might wonder, under our wage gap model, what the average salary is (regardless of gender or height). Or, under our various coin flip models, we might ask what the average number of heads is. More interestingly, we can also ask about averages under some condition: given that the first ten flips were heads, what is the average total number of heads?\n",
    "\n",
    "In these contexts, the word \"average\" is not quite accurate. We can take averages of actual lists of numbers, but our models are not lists of numbers: they are stochastic processes that _produce_ numbers (and other data).\n",
    "\n",
    "What we are really asking about is something called an \"expectation.\" If we have a generative probabilistic model `p`, and a function `f` that takes in samples from `p` and outputs numbers, the _expectation of `f` with respect to the distribution `p`_ is the weighted average of the values taken by `(f x)` as `x` is drawn from the distribution `p`—we weight more probable values of `x` more heavily, and less probable values more lightly. The expectation can be obtained (with probability 1) by taking the limit of the average, as our number of samples goes to infinity, of samples of `(f (p))`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, here is a function `count-heads`, which will play the role of `f`. It takes a sample from one of our coin flip models, and returns the number of heads observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define count-heads\n",
    "  (gen [flips]\n",
    "    (length (filter (gen [x] x) flips))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(count-heads (coin-model-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(count-heads (coin-model-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(count-heads (coin-model-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we've taken one sample of `(f (p))` for each coin flip model `p`. What we'd like to do is take the expectation -- the expected average over infinitely many samples. There are ways to calculate this number exactly; indeed, you probably have a good intuition for what it should be in the three cases above. But in more complex models, that can be harder, so we will focus here on estimation.\n",
    "\n",
    "The simplest technique for estimating an expectation is taking a lot of samples and averaging them. Let's write that up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define avg (gen [l] (clojure.core/float (/ (apply + l) (length l)))))\n",
    "(define mc-expectation\n",
    " (gen [p f n]\n",
    "   (avg (map f (replicate n p)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mc-expectation coin-model-1 count-heads 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mc-expectation coin-model-2 count-heads 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mc-expectation coin-model-3 count-heads 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Expectations of indicator functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use this simple technique to ask about _probabilities_ of events. For example, to estimate the probability under each model of attaining at least 75 heads, we can estimate the expectation of a function that returns one when the condition holds, and zero otherwise. We call this an _indicator_ function.\n",
    "\n",
    "Write a function `indicator` which takes in a `condition` (a predicate that accepts or rejects a given value) and returns an indicator function for that condition (a function that returns 0 or 1 based on whether its input satisfies the condition). Use this to create an indicator for the condition in which there are more than 75 heads in an experiment, and estimate the probability of this event under coin models 1, 2, and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define indicator\n",
    " (gen [condition] (gen [x] (if (condition x) 1 0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mc-expectation coin-model-1 (indicator (gen [l] (>= (count-heads l) 75))) 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mc-expectation coin-model-2 (indicator (gen [l] (>= (count-heads l) 75))) 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mc-expectation coin-model-3 (indicator (gen [l] (>= (count-heads l) 75))) 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Pitfalls of MC estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under what conditions is `mc-expectation` likely to give a misleading answer? (Hint: under `coin-model-1`, is the probability of 75 heads really 0? What is the expected amount won in a lottery?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to apply our `mc-expectation` procedure to our wage-gap model, first to answer the question, \"Under our model, what is the average person's height?\"\n",
    "\n",
    "We immediately hit a snag: last time, the number of heads was a straightforward transformation of `coin-model-i`'s return value. Here, `wage-gap-model` returns only a salary, from which we cannot recover the height of the person whose salary it is.\n",
    "\n",
    "We could alter the model code to return more values—say, `[sex height salary]` as a vector—but there's another way.\n",
    "\n",
    "So far, we've seen only one way of interacting with a Metaprob procedure: running it. But Metaprob provides a lot more flexibility than that, via its powerful `infer` operator.\n",
    "\n",
    "One of the most basic things we can ask `infer` to do for us is _trace the random choices_ made by a Metaprob procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(infer\n",
    "  :procedure wage-gap-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`infer` runs the given procedure and returns three values: the procedure's return value (in this case, a salary), an _execution trace_ that records the random choices made during the procedure's execution, and a \"score\" (which is zero in this case — we'll ignore it for now).\n",
    "\n",
    "Let's look closer at the second return value—the execution trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define [salary tr _]\n",
    " (infer\n",
    "    :procedure wage-gap-model))\n",
    "(smart-plot-trace tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`wage-gap-model` makes three random choices during its execution, and as such, its execution trace records three values. The addresses at which they are stored reflect where in `wage-gap-model`'s soure code they were made. For example, the first recorded value is the choice `:f`, at address `'(0 \"sex\" \"uniform-sample\")`. This indicates that the choice in question was made on line 0, while defining the variable `sex`, using the procedure `uniform-sample`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rewrite our `mc-expectation` procedure to run `f` on the _execution trace_ of `p`, rather than its return value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define mc-expectation-v2\n",
    " (gen [p f n]\n",
    "  (avg\n",
    "    (map f (replicate n\n",
    "      (gen []\n",
    "       (define [_ tr _] (infer :procedure p)) tr))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now answer our question about average height:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define person-height\n",
    "  (gen [t] (trace-get t '(2 \"height\" \"gaussian\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mc-expectation-v2 wage-gap-model person-height 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite close to the true answer of 67.\n",
    "\n",
    "It can be cumbersome to type the addresses manually. Let's give them short names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(addresses-of tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define [sex-adr height-adr salary-adr] (addresses-of tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating an expectation can be useful, but one of the advantages of Bayesian inference is that we can also quantify our uncertainty. By taking a lot of samples and plotting them in a histogram, we can get a sense of what an entire distribution looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(histogram \"Model 1\" \n",
    "  (map count-heads (replicate 1000 coin-model-1)) [0 100] 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(histogram \"Model 2\" (map count-heads (replicate 100 coin-model-2)) [0 100] 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(histogram \"Model 3\" (map count-heads (replicate 400 coin-model-3)) [0 100] 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Expectations under interventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we can sometimes learn interesting things by taking expectations with respect to our models, the most exciting questions usually require _conditioning_ on some piece of data. We might wonder what a person's expected salary is _given that_ she is a woman, or with what probability a person is a woman _given that_ they make $42,000 per year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the simpler of those two questions first. One way to estimate an answer would be to run many simulations in which we force the model to assign to `sex` the value `:f`, then average the resulting salaries. \n",
    "\n",
    "We can _intervene on_ our model and force it to make a certain choice by passing an _intervention trace_ to `infer`. `infer` runs the model as usual, but when it encounters a random choice with an address that appears in the intervention trace, instead of generating a new choice, it simply reuses the one in the intervention trace. \n",
    "\n",
    "In the code below, notice the modified call to `infer`, and also the manner in which we construct an intervention trace using the address of the choice we wish to control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; First, create a version of mc-expectation that accepts an intervention trace\n",
    "(define mc-expectation-v3\n",
    " (gen [p intervene f n]\n",
    "   (avg\n",
    "    (map f (replicate n\n",
    "      (gen []\n",
    "        (define [_ tr _] (infer \n",
    "                           :procedure p \n",
    "                           :intervention-trace intervene))   ; NEW\n",
    "        tr))))))\n",
    "\n",
    "; Create our intervention trace\n",
    "(define ensure-female\n",
    "  (trace-set (trace) sex-adr :f))\n",
    "\n",
    "; Create our accessor (the f to take the expectation of)\n",
    "(define get-salary\n",
    "   (gen [t] (trace-get t salary-adr)))\n",
    "\n",
    "; Run the query\n",
    "(mc-expectation-v3 wage-gap-model ensure-female get-salary 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite a limited technique, however. If we wanted to ask, for instance, about someone's expected height, given that their salary is $42,000, an intervention would not do the trick. Do you see why? Here's what that (wrong) code would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Create our intervention trace\n",
    "(define ensure-42k\n",
    "  (trace-set (trace) salary-adr 42000))\n",
    "\n",
    "; Create our accessor (the f to take the expectation of)\n",
    "(define get-height\n",
    "   (gen [t] (trace-get t height-adr)))\n",
    "\n",
    "; Run the query\n",
    "(mc-expectation-v3 wage-gap-model ensure-42k get-height 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Run the query with NO intervention — just calculates average height, regardless of salary\n",
    "(mc-expectation-v3 wage-gap-model (trace) get-height 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: What queries do interventions answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are these two numbers (almost) the same? Can you characterize the situations when interventions _do_ work as a method of conditioning? Write code for one conditional query that can be answered with interventions, and one that can't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Conditioning with rejection sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to answer more sophisticated conditional queries, we need to turn to another technique: rejection sampling. In rejection sampling, we run our model over and over until we get a sample that satisfies our condition. To estimate a _conditional expectation_ using rejection sampling, we can do this $n$ times, and average the values `f` takes on the $n$ samples that satisfy our condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define rejection-sample\n",
    "  (gen [p condition f]\n",
    "    (define [_ t _] (infer :procedure p))\n",
    "    (if (condition t) (f t) (rejection-sample p condition f))))\n",
    "\n",
    "(define rejection-expectation\n",
    "  (gen [p condition f n]\n",
    "    ; Define a modified version of p which tries again\n",
    "    ; if its first execution doesn't satisfy the condition.\n",
    "    ; Call it n times\n",
    "    (avg (replicate n (gen [] (rejection-sample p condition f))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose we want to know the expected height of someone making $\\$56,000$ — a very high salary under our model. If we expressed our condition as \"a salary of exactly $\\$56,000$\", it would take a very long time to get even one sample. So instead, we ask about a small interval around $\\$56,000$. It is still quite slow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rejection-expectation\n",
    "    wage-gap-model\n",
    "    (gen [t] (< 55500 (trace-get t salary-adr) 56500))\n",
    "    (gen [t] (trace-get t height-adr))\n",
    "    100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we make the same query, but for an interval around $\\$46,000$, it runs much faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rejection-expectation\n",
    "    wage-gap-model\n",
    "    (gen [t] (< 45500 (trace-get t salary-adr) 46500))\n",
    "    (gen [t] (trace-get t height-adr))\n",
    "    100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because our new condition is much more probable. As such, it takes fewer \"tries\" to produce a sample that satisfies it.\n",
    "\n",
    "### Exercise: Estimating running time for rejection sampling\n",
    "Write two rejection sampling queries to estimate (a) the probability that someone making between $\\$45,500$ and $\\$46,500$ is female, and (b) the average height of a woman whose salary is within that range. Before you run each one, try to estimate (roughly, qualitatively) how fast or slow it will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Solution (a)\n",
    "(rejection-expectation\n",
    "    wage-gap-model\n",
    "    (gen [t] (< 45500 (trace-get t salary-adr) 46500))\n",
    "    (indicator (gen [t] (= (trace-get t sex-adr) :f)))\n",
    "    100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Solution (b)\n",
    "(rejection-expectation\n",
    "    wage-gap-model\n",
    "    (gen [t] (and (= (trace-get t sex-adr) :f) (< 45500 (trace-get t salary-adr) 46500)))\n",
    "    (gen [t] (trace-get t height-adr))\n",
    "    100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With rejection sampling, we can begin to see the first applications of probabilistic modeling to _data analysis_. The idea is that we _condition_ our model's execution on the actual data we've observed, then ask questions about the _latent variables_ — the pieces we may not have observed directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one example, consider the following model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define hybrid-coin-model\n",
    "  (gen []\n",
    "    (define which-model (uniform-sample [coin-model-1 coin-model-2 coin-model-3]))\n",
    "    (which-model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we saw 100 heads, we can now ask about likely values of `which-model`. This allows us to pose questions like: if I saw 100 heads, which of my three possible explanations was most likely? And what are the chances the coin comes up heads next time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define count-heads-in-trace\n",
    "  (gen [t]\n",
    "    (apply + (map (gen [adr] (if (and (boolean? (trace-get t adr)) (trace-get t adr)) 1 0))\n",
    "                  (addresses-of t)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Probability of Coin Model 3 if I saw 100 heads:\n",
    "(rejection-expectation\n",
    "    hybrid-coin-model\n",
    "    (gen [t] (= (count-heads-in-trace t) 100))\n",
    "    (indicator (gen [t] (= coin-model-3 (trace-get t '(0 \"which-model\" \"uniform-sample\")))))\n",
    "    100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Estimated probability of next coin flip being heads if I saw 100 heads\n",
    "(rejection-expectation\n",
    "    hybrid-coin-model\n",
    "    (gen [t] (= (count-heads-in-trace t) 100))\n",
    "    (gen [t]\n",
    "      (define actual-model (trace-get t '(0 \"which-model\" \"uniform-sample\")))\n",
    "      (cond\n",
    "        (= actual-model coin-model-1) 0.5\n",
    "        (= actual-model coin-model-2) (trace-get t '(1 \"which-model\" 0 \"p\" \"uniform\"))\n",
    "        (= actual-model coin-model-3) (trace-get t '(1 \"which-model\" 0 \"p\" \"uniform-sample\"))))\n",
    "    100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is (in most runs) _almost_ 1, but indicates that there is still some uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Aside: The posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have been making \"point estimates\" to answer our queries, summarizing our answers as a single number. For example, suppose suppose I tell you I have flipped a coin 100 times and it came up heads more than sixty times. I ask you to guess how many times exactly it came up heads. Using the `hybrid-coin-model`, you could compute an expectation, which is one way to generate a single-number answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rejection-expectation\n",
    "    hybrid-coin-model\n",
    "    (gen [t] (> (count-heads-in-trace t) 60))\n",
    "    count-heads-in-trace\n",
    "    100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But one of the main advantages of the sort of probabilistic modeling we're doing is that we have access to much richer information. \n",
    "\n",
    "To introduce some terminology, the process of \"Bayesian data analysis\" consists of the following steps:\n",
    "\n",
    "*Step 1*. First, express our prior beliefs about a generative process by encoding them into a model. The distribution over traces induced by that model is called the _prior_: it encodes our beliefs _prior_ to seeing any data. In our case, the prior distribution over \"number of heads\" looks like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(histogram \"Prior\" (replicate 500 (gen [] (count-heads (hybrid-coin-model)))) [0 100] 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see by examining this plot, although we believe that any number of heads is theoretically possible, we would be much more surprised to see, say, 75 heads than 0, 50, or 100. We can also plot our prior beliefs about which of the three models is being used, and check that all three are basically equally likely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define model-number\n",
    " (gen [t] \n",
    "      (define which (trace-get t '(0 \"which-model\" \"uniform-sample\")))\n",
    "      (cond \n",
    "        (= which coin-model-1) 1\n",
    "        (= which coin-model-2) 2\n",
    "        (= which coin-model-3) 3)))\n",
    "\n",
    "(histogram \"Prior on model choice\" (replicate 500 (gen [] (model-number (nth (infer :procedure hybrid-coin-model) 1)))) [1 3] 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 2.* Condition on observed data, retrieving the _posterior distribution_ -- like the prior distribution, but updated to reflect our new beliefs after seeing the data. In this case, we can observe that over 60 coins came up heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define plot-posterior-rejection\n",
    "  (gen [p condition f n [min-val max-val] step]\n",
    "    (histogram \"Posterior\" (replicate n (gen [] (rejection-sample p condition f))) [min-val max-val] step)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(plot-posterior-rejection hybrid-coin-model (gen [t] (> (count-heads-in-trace t) 60)) count-heads-in-trace 100 [0 100] 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our point estimate above will in most cases be quite misleading!\n",
    "\n",
    "We can also plot the posterior on `which-model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(plot-posterior-rejection hybrid-coin-model (gen [t] (> (count-heads-in-trace t) 60)) model-number 500 [1 3] 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Plotting the posterior\n",
    "Write a query that plots the salary distribution for people over 70 inches tall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Solution\n",
    "(plot-posterior-rejection\n",
    "  wage-gap-model\n",
    "  (gen [t] (> (trace-get t height-adr) 70))\n",
    "  (gen [t] (trace-get t salary-adr))\n",
    "  300\n",
    "  [30000 60000] 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Conditioning with likelihood weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last two sections, we saw two ways of generating _conditional_ samples:\n",
    "\n",
    "1. Intervening on our model to force the \"random choices\" of interest to take on the values we want them to. This is fast, but is only correct in select cases, where the random choices we're conditioning on don't depend on any prior random choices.\n",
    "\n",
    "2. Rejection sampling. This gives exact samples from the posterior; the only problem is how slow it is.\n",
    "\n",
    "We'd like the speed of (1) with the correctness of (2). As a step toward getting there, let's think a bit more about why exactly (1) is usually incorrect.\n",
    "\n",
    "Suppose we are trying to estimate the expected salary for someone who is 70 inches tall (5' 10\"). We can get a (slow) estimate via rejection sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rejection-expectation\n",
    "    wage-gap-model\n",
    "    (gen [t] (< 69.9 (trace-get t height-adr) 70.1))\n",
    "    (gen [t] (trace-get t salary-adr))\n",
    "    100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do the same using our intervention trace method, we get a fast but inaccurate answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mc-expectation-v3\n",
    "    wage-gap-model\n",
    "    (trace-set (trace) height-adr 70)\n",
    "    (gen [t] (trace-get t salary-adr))\n",
    "    100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the answer lower in the intervention trace method? Under our model, men are more likely to be 70 inches tall than women, so if we know someone is 70 inches tall, we might expect it's more likely for them to be a man, and—incorporating the wage gap—we might expect their salary to be higher. The rejection sampling algorithm takes this into account. It simply waits for 100 samples where the height was 70 inches, the majority of which will be men; the salary will have been higher for those samples, so the expected salary will be higher.\n",
    "\n",
    "The intervention method, on the other hand, is impatient, and immediately sets height to 70 in every run, severing the tie between gender and height. That means that our collection of 100 samples will contain just as many 70-inch-tall women as 70-inch-tall men, which is improbable under our model.\n",
    "\n",
    "What if we made a compromise? We still artificially set the height of each sample to 70, but also measure how likely the model _would have been_ to choose 70 anyway—a number that will be higher for `sex=:m` samples than for `sex=:f` samples. (For those familiar with probability theory, we're talking about probability densities: `p(height=70|sex)`.) At the end, we take a _weighted average_ of the salaries, where each salary is weighted by the likelihood that the employee really would have been 70 inches (given the random choices we'd already made about them: their sex)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement this, `infer` has one final trick up its sleeve: we can provide a _target trace_, which works just like an intervention trace, but also causes the constrained choices to be _scored_. For each targeted choice, Metaprob computes the probability that it _would have_ made the same choice under the prior, and accumulates these numbers (as a sum of _log_ probabilities) into a _score_ that is returned, along with the model's output and execution trace, from `infer`.\n",
    "\n",
    "We can use this feature to implement the likelihood weighting algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Note -- we exponentiate the scores, because they are returned as log probabilities\n",
    "(define weighted-samples\n",
    "  (gen [proc target f n]\n",
    "    (define samples (replicate n (gen [] (infer :procedure proc :target-trace target))))\n",
    "    (map (gen [[o t s]] \n",
    "              [(f t) (exp s)]) samples)))\n",
    "\n",
    "(define lh-weighting\n",
    "   (gen [proc target f n]\n",
    "    (define samples (weighted-samples proc target f n))\n",
    "    (/ (apply + (map (gen [[v s]] (* v s)) samples))\n",
    "       (apply + (map (gen [[_ s]] s) samples)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the technique to the question we explored above, to estimate the expected salary for someone who is 70 inches tall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lh-weighting wage-gap-model\n",
    "    (trace-set (trace) height-adr 70)\n",
    "    (gen [t] (trace-get t salary-adr))\n",
    "    100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This brings us quite close to the rejection-sampled estimate, and much faster!\n",
    "\n",
    "### 3.5.1 Sampling/importance resampling\n",
    "This same idea can be used to produce (and plot) samples from the posterior. The trick is this: to generate a single sample from the (approximate) posterior, we sample some number $n$ of \"particles\" with likelihood weights. We then choose one of them at random, giving each one probability proportional to its score. This variant of the technique is called \"sampling/importancee resampling.\" Let's implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define sample-importance-resample\n",
    " (gen \n",
    "   [proc target f n-particles]\n",
    "    ; Generate particles\n",
    "    (define particles (weighted-samples proc target f n-particles))\n",
    "    ; Choose one at random, with prob. proportional to importance weight\n",
    "    (define sum-of-weights (apply + (map (gen [[_ s]] s) particles)))\n",
    "    (define which-particle (categorical (map (gen [[o s]] (/ s (+ 1e-10 sum-of-weights))) particles)))\n",
    "    ; Return only the sampled value (not the score)\n",
    "    (define [sampled-value _] (nth particles which-particle))\n",
    "    sampled-value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define plot-posterior-importance\n",
    "  (gen [p target f particles-per-sample n [min-val max-val] step]\n",
    "    (histogram \"Posterior (Importance Resampling)\" (replicate n (gen [] (sample-importance-resample p target f particles-per-sample))) [min-val max-val] step)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last time, we consider the problem of estimating someone's salary given that they are 73 inches tall. This time, instead of using a point estimate, we'll plot the entire posterior, which will give us a better sense of how salaries might vary within the population of 73-inch-tall people.\n",
    "\n",
    "Let's first plot using only one particle. In this case, we are not getting the posterior at all: we are just sampling from the prior with an intervention (setting height=73)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; 1 particle -- sampling from the (intervened) prior\n",
    "(plot-posterior-importance \n",
    "  wage-gap-model\n",
    "  (trace-set (trace) height-adr 73)\n",
    "  (gen [t] (trace-get t salary-adr))\n",
    "   1 1000\n",
    "  [30000 60000] 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 5 particles per sample, we get a better approximation to the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; 5 particles -- better approx to posterior\n",
    "(plot-posterior-importance \n",
    "  wage-gap-model\n",
    "  (trace-set (trace) height-adr 73)\n",
    "  (gen [t] (trace-get t salary-adr))\n",
    "   5 500\n",
    "  [30000 60000] 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check our work, we can revert to the rejection sampling technique to get exact samples from the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(plot-posterior-rejection\n",
    "    wage-gap-model\n",
    "    (gen [t] (< 72.9 (trace-get t height-adr) 73.1))\n",
    "    (gen [t] (trace-get t salary-adr))\n",
    "    500\n",
    "    [30000 60000] 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty similar. The upshot is that the sampling/importance resampling method gives us an accurate picture of the posterior in a much more efficient manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll review all the techniques we've learned on a new example model, and look at ways of extending our models with model-specific \"custom inference procedures.\"\n",
    "\n",
    "The model we're using in this section is a _bivariate Gaussian_. A bivariate Gaussian models pairs of variables that each follow a \"bell curve,\" but are correllated with one another — for example, a person's height and weight. \n",
    "\n",
    "Ignoring weight, people's heights are distributed normally, with a high probability at the mean and lower probability the further you get away from the mean. Here we plot samples from a _univariate_ Gaussian with mean 70 and _standard deviation_ 3; standard deviation is a measure of the \"spread\" of a distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(histogram-with-curve\n",
    "    \"Gaussian (height)\"\n",
    "    (replicate 5000 (gen [] (gaussian 70 3)))\n",
    "    (gen [x] (exp (score-gaussian x [70 3])))\n",
    "    [50 90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And weight may also be distributed normally, with different mean and standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(histogram-with-curve \"Gaussian (weight)\"\n",
    "           (replicate 5000 (gen [] (gaussian 150 20))) \n",
    "           (gen [x] (exp (score-gaussian x [150 20])))\n",
    "           [80 220])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So one idea for how to model a person's height and weight would be to draw them individually from these distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define person-v1\n",
    "  (gen [] \n",
    "    (define height (gaussian 70 3))\n",
    "    (define weight (gaussian 150 20))\n",
    "    [height weight]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there's something wrong about this model: the height and weight are totally independent. In real life, a tall person is likely to weigh more; in our model, this is not the case.\n",
    "\n",
    "Enter the _bivariate Gaussian_: in addition to a pair of means, it also takes a _covariance matrix_, a symmetric 2x2 matrix $\\Sigma$, where $\\sigma_{11}$ and $\\sigma_{22}$ are the variances (squared standard deviations) of variables 1 and 2, but $\\sigma_{12} = \\sigma_{21}$ is the _covariance_ of the variables with one another. A positive covariance indicates direct correlation; a negative covariance indicates inverse correlation. Zero covariance means the variables are completely independent. (Note: in general, zero covariance does _not_ imply independence, but in a bivariate Gaussian model, it does.)\n",
    "\n",
    "There are a number of ways to sample from a bivariate Gaussian. Below, we model the distribution as follows: first, we generate the first variable according to its mean and standard deviation (e.g., generate a height). Then, we figure out (based on how far that sample was from the mean, and the covariance), the expectation of the second variable (e.g., given the height we generated, what would we expect the weight to be?). Using the variance and covariance information, we can also calculate how much of a spread we expect to find around that expected value. Now that we have a new mean and covariance, we can sample the second variable (generate a weight). Here's the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define conditional-biv-gaussian-mean\n",
    "  (gen [x1 [[mu1 mu2] [[sigma_11 sigma_12] [sigma_21 sigma_22]]]]\n",
    "    (+ mu2 (* (- x1 mu1) (/ sigma_12 sigma_11)))))\n",
    "\n",
    "(define conditional-biv-gaussian-variance\n",
    "  (gen [x1 [[mu1 mu2] [[sigma_11 sigma_12] [sigma_21 sigma_22]]]]\n",
    "    (/ (- (* sigma_11 sigma_22) (* sigma_12 sigma_12)) sigma_11)))\n",
    "\n",
    "(define conditional-biv-gaussian-density\n",
    "  (gen [x1 params]\n",
    "    (gen [x]\n",
    "    (exp (score-gaussian x [(conditional-biv-gaussian-mean x1 params)\n",
    "                            (sqrt (conditional-biv-gaussian-variance x1 params))])))))\n",
    "\n",
    "(define biv-gauss\n",
    "  (gen [[mu1 mu2] [[sigma_11 sigma_12] [sigma_21 sigma_22]]]\n",
    "       ; draw the first variable normally \n",
    "       (define x1 (gaussian mu1 (sqrt sigma_11)))\n",
    "       ; calculate the new mean and variance of the second variable\n",
    "       (define x2_mean\n",
    "           (conditional-biv-gaussian-mean \n",
    "               x1\n",
    "               [[mu1 mu2] [[sigma_11 sigma_12] [sigma_21 sigma_22]]]))\n",
    "       (define x2_var\n",
    "           (conditional-biv-gaussian-variance \n",
    "               x1\n",
    "               [[mu1 mu2] [[sigma_11 sigma_12] [sigma_21 sigma_22]]]))\n",
    "       ; draw the second variable\n",
    "       (define x2 (gaussian x2_mean (sqrt x2_var)))\n",
    "       ; return both\n",
    "       [x1 x2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define person-v2\n",
    "  (gen []\n",
    "    ; 9 = 3^2, 400 = 20^2; we pass variance instead of standard deviation here\n",
    "    ; setting covariance to 0 (instead of 40) would exactly recover the person-v1 model above.\n",
    "    (biv-gauss [70 150] [[9 40] [40 400]])))\n",
    "(define [height-adr weight-adr] (addresses-of (nth (infer :procedure person-v2 :inputs []) 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a scatter plot to visualize the sorts of samples this generates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(custom-scatter-plot \"Bivariate Gaussian\"\n",
    "  (replicate 1000 person-v2)\n",
    "  \"cross\" \"white\" \"blue\" [[60 80] [90 210]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"shape\" of this data is elliptical: there is a rotated ellipse in which the points are concentrated, with density decreasing as we move away from the center of the ellipse along either of its axes. We can see this better by plotting the actual probability density; to do so, we write an assessor function that exactly computes the probability density of a specific point. Graphing that function in a contour plot can help us better understand how the idea of a \"bell curve\" generalizes to two dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define biv-gaussian-density\n",
    "  (gen [[mu1 mu2] [[s11 s12] [s21 s22]]]\n",
    "    (gen [x y]\n",
    "      (define x-from-mu (- x mu1))\n",
    "      (define y-from-mu (- y mu2))\n",
    "      (/\n",
    "       (exp\n",
    "        (/\n",
    "          (- (+\n",
    "              (/ (* x-from-mu x-from-mu) s11)\n",
    "              (/ (* y-from-mu y-from-mu) s22))     \n",
    "            (/ (* 2 s12 x-from-mu y-from-mu) (* s11 s22)))\n",
    "          (* -2 (/ (- (* s11 s22) (* s12 s12)) (* s11 s22)))))\n",
    "        (* 2 3.1415926 (sqrt (- (* s11 s22) (* s12 s12))))))))\n",
    "\n",
    "(scatter-with-contours\n",
    "  \"Bivariate Gaussian\"\n",
    "  (replicate 100 person-v2); [] ;(replicate 100 (gen [] (biv-gauss [0 0] [[1/5 0] [0 1/5]])))\n",
    "  (biv-gaussian-density [70 150] [[9 40] [40 400]])\n",
    "  [[60 80] [90 210]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot, blue regions correspond to areas of low probability, and red regions to areas of high probability. Equiprobability contours are ellipses. The wider or taller the ellipses, the more uncertainty there is. The \"off-center\" rotation you see tells us that variables $x$ and $y$ are not independent: if we know one, it changes our guess about the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feel for what the parameters of the bivariate Gaussian means, let's look at how the contour plots change as we change the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Changing the means moves the ellipse\n",
    "(scatter-with-contours\n",
    "  \"Bivariate Gaussian\"\n",
    "  []\n",
    "  (biv-gaussian-density [65 120] [[9 40] [40 400]])\n",
    "  [[60 80] [90 210]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Increasing sigma_11 increases the spread in the x direction.\n",
    "(scatter-with-contours\n",
    "  \"Bivariate Gaussian\"\n",
    "  []\n",
    "  (biv-gaussian-density [70 150] [[18 40] [40 400]])\n",
    "  [[60 80] [90 210]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Increasing sigma_22 increases the spread in the y direction.\n",
    "(scatter-with-contours\n",
    "  \"Bivariate Gaussian\"\n",
    "  []\n",
    "  (biv-gaussian-density [70 150] [[9 40] [40 800]])\n",
    "  [[60 80] [90 210]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Decreasing sigma_12 decreases the correlation between the variables, leading to a \"flatter\" ellipse\n",
    "(scatter-with-contours\n",
    "  \"Bivariate Gaussian\"\n",
    "  []\n",
    "  (biv-gaussian-density [70 150] [[9 0] [0 400]])\n",
    "  [[60 80] [90 210]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in this last plot that no matter what $x$ is, we induce the same distribution on $y$ (and vice versa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, going back to our model of height and weight, suppose we know someone's weight and want to guess their height. We could use rejection sampling, but it's slow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Height samples for someone whose weight is 175\n",
    "(define conditional-samples\n",
    "  (replicate 100 \n",
    "    (gen []\n",
    "      (rejection-sample \n",
    "        person-v2\n",
    "        (gen [t] (< 174.5 (trace-get t weight-adr) 175.5))\n",
    "        (gen [t] [(trace-get t height-adr) (trace-get t weight-adr)])))))\n",
    "\n",
    "(histogram-with-curve\n",
    "  \"Exact posterior and rejection samples of height\"\n",
    "  (map (gen [[h w]] h) conditional-samples)\n",
    "  (conditional-biv-gaussian-density 175 [[150 70] [[400 40] [40 9]]])\n",
    "  [60 80])\n",
    "\n",
    ";; (scatter-with-contours \n",
    ";;     \"Posterior samples of height\"\n",
    ";;     conditional-samples\n",
    ";;     (biv-gaussian-density [70 150] [[9 40] [40 400]])\n",
    ";;     [[60 80] [174.4 175.6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this histogram, we've also plotted the correct probability density for the posterior distribution. Generally, it is not tractable to compute exact posteriors—otherwise approximate inference techniques like the ones we've been exploring would be unnecessary. But for this simple case, of a bivariate Gaussian, it is possible, and we use the analytically computed curve to evaluate how well our sampling technique approximates the posterior distribution. \n",
    "\n",
    "We can try to fix the speed problem with sampling/importance resampling. If we use only one particle, we are essentially disregarding the condition and sampling from the prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Height samples for someone whose weight is 175\n",
    "(define sir-samples-1-particle\n",
    "  (replicate 100 \n",
    "    (gen [] \n",
    "      (sample-importance-resample \n",
    "          person-v2\n",
    "          (trace-set (trace) weight-adr 175)\n",
    "          (gen [t] [(trace-get t height-adr) (trace-get t weight-adr)])\n",
    "          1))))\n",
    "\n",
    "\n",
    "(histogram-with-curve\n",
    "  \"Exact posterior and prior samples of height\"\n",
    "  (map (gen [[h w]] h) sir-samples-1-particle)\n",
    "  (conditional-biv-gaussian-density 175 [[150 70] [[400 40] [40 9]]])\n",
    "  [60 80])\n",
    "\n",
    ";; (scatter-with-contours \n",
    ";;     \"Prior samples of height\"\n",
    ";;     isr-samples-1-particle\n",
    ";;     (biv-gaussian-density [70 150] [[9 40] [40 400]])\n",
    ";;     [[60 80] [174.8 175.2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can increase the number of particles and shift the distribution toward the true posterior, at a cost of spending more time per sample. See if you can play with the number of particles (below, 5) to achieve a good trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; Height samples for someone whose weight is 175\n",
    "(define sir-samples-multiple-particles\n",
    "  (replicate 100 \n",
    "    (gen [] \n",
    "      (sample-importance-resample \n",
    "          person-v2\n",
    "          (trace-set (trace) weight-adr 175)\n",
    "          (gen [t] [(trace-get t height-adr) (trace-get t weight-adr)])\n",
    "          5))))\n",
    "\n",
    "\n",
    "(histogram-with-curve\n",
    "  \"Exact posterior and approximate importance samples of height\"\n",
    "  (map (gen [[h w]] h) sir-samples-multiple-particles)\n",
    "  (conditional-biv-gaussian-density 175 [[150 70] [[400 40] [40 9]]])\n",
    "  [60 80])\n",
    "\n",
    ";; (scatter-with-contours \n",
    ";;     \"Approximate posterior samples of height\"\n",
    ";;     isr-samples-10-particles\n",
    ";;     (biv-gaussian-density [70 150] [[9 40] [40 400]])\n",
    ";;     [[60 80] [174.8 175.2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6.1 Custom inference procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the bivariate Gaussian, we happen to know exactly the mathematical form of the posterior. That is, for our person model, we can calculate the exact distribution that someone's height is drawn from when we know their weight. It is silly to use approximate algorithms when an exact one exists.\n",
    "\n",
    "Metaprob is designed to be easily extensible, and to allow practitioners to encode any special knowledge like this directly into their models. It supports a class of user-defined procedures called \"custom inference procedures,\" which specify not only a generative model, but also a custom \"implementation\" that overrides Metaprob's default behavior in the presence of target or intervention traces. In other words, we can specify custom behavior for when `(infer ...)` is called on our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; We use `inf` to define a custom inference procedure. It takes in:\n",
    "; -- a name\n",
    "; -- a model\n",
    "; -- an \"implementation\"\n",
    "(define custom-biv-gauss\n",
    "  (inf \"biv-gauss\" ; name\n",
    "       biv-gauss   ; model\n",
    "       ; implementation:\n",
    "       ; accepts four arguments. \n",
    "       ;    1. inputs to the model (biv gaussian params)\n",
    "       ;    2. an intervention trace\n",
    "       ;    3. a target trace\n",
    "       ;    4. a boolean `out?` that specifies whether to return an output\n",
    "       ;       trace.\n",
    "       ; returns three things:\n",
    "       ;    1. the output of the model on these inputs\n",
    "       ;    2. if out? is true, an output trace consistent with intervene and\n",
    "       ;        target.\n",
    "       ;    3. a score, the log of a number proportional to the _ratio_ between the true posterior density\n",
    "       ;       and the density (at the sampled trace) of the distribution from which our implementation samples.\n",
    "       ;       Because we are sampling from the exact posterior here, the ratio is 1, and so\n",
    "       ;       we return log(1) == 0.\n",
    "       (gen [[[mu1 mu2] [[s11 s12] [s21 s22]]] intervene target out?]\n",
    "         \n",
    "        ; read x1 from intervention or target trace, or, if it's not specified\n",
    "        ; but x2 is in the target trace, sample it from the exact posterior.\n",
    "        ; if neither x1 nor x2 are constrained, sample a fresh x1 just like\n",
    "        ; the biv-gauss model does.\n",
    "        (define x1\n",
    "          (cond\n",
    "            (trace-has? intervene '(0 \"x1\" \"gaussian\")) (trace-get intervene '(0 \"x1\" \"gaussian\"))\n",
    "            (trace-has? target '(0 \"x1\" \"gaussian\")) (trace-get target '(0 \"x1\" \"gaussian\"))\n",
    "            (trace-has? target '(3 \"x2\" \"gaussian\"))\n",
    "              (gaussian (conditional-biv-gaussian-mean (trace-get target '(3 \"x2\" \"gaussian\"))\n",
    "                                                       [[mu2 mu1] [[s22 s12] [s21 s11]]])\n",
    "                        (sqrt (conditional-biv-gaussian-variance (trace-get target '(3 \"x2\" \"gaussian\"))\n",
    "                                                           [[mu2 mu1] [[s22 s12] [s21 s11]]])))\n",
    "            \n",
    "            true (gaussian mu1 (sqrt s11))))\n",
    "         \n",
    "         ; Either use the provided value for x2, or sample conditionally based on x1.\n",
    "         (define x2\n",
    "            (cond\n",
    "              (trace-has? intervene '(3 \"x2\" \"gaussian\")) (trace-get intervene '(3 \"x2\" \"gaussian\"))\n",
    "              (trace-has? target '(3 \"x2\" \"gaussian\")) (trace-get target '(3 \"x2\" \"gaussian\"))\n",
    "              true\n",
    "                (gaussian (conditional-biv-gaussian-mean x1\n",
    "                                                       [[mu1 mu2] [[s11 s12] [s21 s22]]])\n",
    "                        (sqrt (conditional-biv-gaussian-variance x1\n",
    "                                                           [[mu1 mu2] [[s11 s12] [s21 s22]]])))))\n",
    "        \n",
    "        ; Return values\n",
    "        [[x1 x2]\n",
    "         (if out?\n",
    "             (trace-set (trace-set (trace) '(0 \"x1\" \"gaussian\") x1) '(3 \"x2\" \"gaussian\") x2)\n",
    "             (trace))\n",
    "        0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this, sampling/importance resampling will \"just work,\" no matter how many particles we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; rewrite person-v2 using custom-biv-gauss instead of biv-gauss\n",
    "(define person-v3\n",
    "  (gen []\n",
    "    (custom-biv-gauss [70 150] [[9 40] [40 400]])))\n",
    "(define [height-adr weight-adr] (addresses-of (nth (infer :procedure person-v3) 1)))\n",
    "\n",
    "\n",
    "; Height samples for someone whose weight is 170\n",
    "(define sir-samples-1-particle\n",
    "  (replicate 100 \n",
    "    (gen [] \n",
    "      (sample-importance-resample \n",
    "          person-v3\n",
    "          (trace-set (trace) weight-adr 175)\n",
    "          (gen [t] [(trace-get t height-adr) (trace-get t weight-adr)])\n",
    "          1))))\n",
    "\n",
    "\n",
    "(histogram-with-curve\n",
    "  \"Exact posterior with fast exact samples\"\n",
    "  (map (gen [[h w]] h) sir-samples-1-particle)\n",
    "  (conditional-biv-gaussian-density 175 [[150 70] [[400 40] [40 9]]])\n",
    "  [60 80])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2 Using the bivariate Gaussian as part of another model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the bivariate Gaussian to help implement a new model. We will model the following strange situation. Suppose we go to the circus and see an act in which two brothers have a circus act; they pretend to be one very tall person by standing on top of one another and wearing a large coat. From our seats, we can estimate (perhaps with some error) the total height of both brothers, but would like to infer their individual heights. Here is the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define circus-brothers\n",
    "  (gen []\n",
    "    (define [h1 h2]\n",
    "      (custom-biv-gauss [70 70] [[9 5] [5 9]]))\n",
    "    (define total-height\n",
    "        (gaussian (+ h1 h2) 3))\n",
    "    total-height))\n",
    "\n",
    "(define [h1-adr h2-adr total-adr] (addresses-of (nth (infer :procedure circus-brothers) 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(scatter-with-contours\n",
    "  \"Prior distribution of brothers' heights\"\n",
    "  []\n",
    "  (biv-gaussian-density [70 70] [[9 5] [5 9]])\n",
    "  [[60 80] [60 80]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define observed-height 155)\n",
    "(scatter-with-contours\n",
    "  \"Posterior distribution of brothers' heights\"\n",
    "  []\n",
    "  (biv-gaussian-density [(+ 17.02 (* 0.378 observed-height)) (+ 17.02 (* 0.378 observed-height))] [[3.7 -0.2966] [-0.2966 3.7]])\n",
    "  [[60 80] [60 80]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shape should make some sense: because 155 is a bit higher than what we'd ordinarily expect, the distribution has moved up and to the right. It's also straightened out: originally, knowing that one brother was taller would also make us think the second brother was taller, because brothers tend to have similar heights. But now, given that their total height is constrained to be somewhere near 155, learning that one brother is very tall would lead us to believe the other brother must be a bit shorter (and vice versa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some samples, using all our techniques for sampling from a conditional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define extract-heights\n",
    "  (gen [t] [(trace-get t h1-adr) (trace-get t h2-adr)]))\n",
    "\n",
    "; From the prior\n",
    "(define prior-samples\n",
    "  (replicate 100 \n",
    "    (gen [] \n",
    "      (sample-importance-resample\n",
    "          circus-brothers\n",
    "          (trace-set (trace) total-adr observed-height)\n",
    "          extract-heights\n",
    "          1))))\n",
    "\n",
    "(scatter-with-contours\n",
    "  \"Prior samples of brothers' heights\"\n",
    "  prior-samples\n",
    "  (biv-gaussian-density [(+ 17.02 (* 0.378 observed-height)) (+ 17.02 (* 0.378 observed-height))] [[3.7 -0.2966] [-0.2966 3.7]])\n",
    "  [[60 80] [60 80]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; From the approximate posterior\n",
    "(define approx-posterior-samples\n",
    "  (replicate 100 \n",
    "    (gen [] \n",
    "      (sample-importance-resample\n",
    "          circus-brothers\n",
    "          (trace-set (trace) total-adr observed-height)\n",
    "          extract-heights\n",
    "          10))))\n",
    "\n",
    "(scatter-with-contours\n",
    "  \"Approx. posterior samples of brothers' heights\"\n",
    "  approx-posterior-samples\n",
    "  (biv-gaussian-density [(+ 17.02 (* 0.378 observed-height)) (+ 17.02 (* 0.378 observed-height))] [[3.7 -0.2966] [-0.2966 3.7]])\n",
    "  [[60 80] [60 80]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; From the posterior but slowly\n",
    "(define posterior-samples\n",
    "  (replicate 100 \n",
    "    (gen [] \n",
    "      (rejection-sample \n",
    "          circus-brothers\n",
    "          (gen [t]\n",
    "               (< -0.5\n",
    "                  (- observed-height (trace-get t total-adr))\n",
    "                  0.5))\n",
    "          extract-heights))))\n",
    "\n",
    "(scatter-with-contours\n",
    "  \"Posterior samples of brothers' heights\"\n",
    "  posterior-samples\n",
    "  (biv-gaussian-density [(+ 17.02 (* 0.378 observed-height)) (+ 17.02 (* 0.378 observed-height))] [[3.7 -0.2966] [-0.2966 3.7]])\n",
    "  [[60 80] [60 80]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One technique we can use to get more accurate samples than sampling/importance resampling in less time than rejection sampling is to use \"Markov Chain Monte Carlo\" (MCMC) methods. These methods involve starting with a single sample and iteratively changing it, in such a way that eventually, each successive iteration yields a sample from the posterior. There are many ways to do this, but below, we use an algorithm called Single-Site Metropolis Hastings, which (in our case) will iteratively propose changes to one brother's height at a time, then randomly deciding to either accept or reject the proposal. (The probability with which a proposal is accepted depends on a couple factors, including how likely our constraint is given the proposal.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define initial-sample\n",
    "  ((infer :procedure circus-brothers\n",
    "         :target-trace (trace-set (trace) total-adr 155)) 1))\n",
    "\n",
    "(define mh-traj\n",
    "    (clojure.core/reduce\n",
    "      (gen [samples _] \n",
    "           (clojure.core/cons\n",
    "               (immutable-single-site-metropolis-hastings-step\n",
    "                           circus-brothers []\n",
    "                           (first samples) (list total-adr))\n",
    "       samples))\n",
    "  (list initial-sample)\n",
    "  (range 500)))\n",
    "\n",
    "(scatter-with-contours\n",
    "  \"Posterior samples of brothers' heights\"\n",
    "  (map extract-heights mh-traj)\n",
    "  (biv-gaussian-density [(+ 17.02 (* 0.378 observed-height)) (+ 17.02 (* 0.378 observed-height))] [[3.7 -0.2966] [-0.2966 3.7]])\n",
    "  [[60 80] [60 80]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click Play in the animated plot below to view the trajectory. It flashes red whenever the trajectory stays stationary because of a rejected proposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mh-animation \"Posterior samples of brothers' heights\"\n",
    "              (clojure.core/reverse (map extract-heights mh-traj))\n",
    "              [[60 80] [60 80]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge Exercise: Custom inference for circus brothers\n",
    "Implement a custom inference procedure for `circus-brothers`, using the fact that conditioned on a certain observed total height $h$, the brothers' individual heights are drawn from a bivariate Gaussian:\n",
    "- with means $\\mu_1 = \\mu_2 = 17.02 + 0.378h$, \n",
    "- with each height having marginal variance $3.7$, and\n",
    "- with the covariance between the two heights equal to $-0.2966$. \n",
    "\n",
    "Plot importance samples and MH samples from this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(smart-plot-trace (nth (infer :procedure circus-brothers) 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(smart-plot-trace (nth (infer :procedure custom-biv-gauss :inputs [[0 0] [[1 0] [0 1]]]) 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[h1-adr h2-adr total-adr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(define trace-subtrace-or-empty\n",
    "    (gen [t a]\n",
    "      (if (trace-has-subtrace? t a) (trace-subtrace t a) (trace))))\n",
    "\n",
    "(define custom-circus-brothers\n",
    "  (inf \"circus-brothers\"\n",
    "       circus-brothers\n",
    "       (gen [[] intervene target out?]\n",
    "        ; h1 h2\n",
    "        (define [[h1 h2] o _]\n",
    "          (infer :procedure custom-biv-gauss\n",
    "                   :intervention-trace (trace-subtrace-or-empty intervene '(0 \"definiens\" \"custom-biv-gauss\"))\n",
    "                   :target-trace       (trace-subtrace-or-empty target '(0 \"definiens\" \"custom-biv-gauss\"))\n",
    "                   :inputs \n",
    "                 (if (trace-has? target total-adr)\n",
    "                     [[(+ 17.02 (* 0.378 (trace-get target total-adr))) (+ 17.02 (* 0.378 (trace-get target total-adr)))] \n",
    "                      [[3.7 -0.2966] [-0.2966 3.7]]]\n",
    "                     [[70 70]\n",
    "                      [[9 5] [5 9]]])))\n",
    "        ; total height\n",
    "        (define total-height\n",
    "            (cond\n",
    "                (trace-has? intervene total-adr) (trace-get intervene total-adr)\n",
    "                (trace-has? target total-adr)    (trace-get target total-adr)\n",
    "                true (gaussian (+ h1 h2) 3)))\n",
    "            \n",
    "        [total-height (trace-set-values-at (trace)\n",
    "                        total-adr total-height h1-adr h1 h2-adr h2)\n",
    "         0])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lein-Clojure",
   "language": "clojure",
   "name": "lein-clojure"
  },
  "language_info": {
   "file_extension": ".clj",
   "mimetype": "text/x-clojure",
   "name": "clojure",
   "version": "1.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
